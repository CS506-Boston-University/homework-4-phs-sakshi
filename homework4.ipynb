{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 (100 Points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (40pts)\n",
    "\n",
    "In this exercise we will implement a Natural Language Processing (NLP) system using binary logistic regression.\n",
    "\n",
    "The data you will be working with comes from the [Yelp Dataset](https://www.yelp.com/dataset). Each line is a review that consists of a label (0 for negative reviews and 1 for positive reviews) and a set of words.\n",
    "\n",
    "```\n",
    "1 i will never forget this single breakfast experience in mad...\n",
    "0 the search for decent chinese takeout in madison continues ...\n",
    "0 sorry but me julio fell way below the standard even for med...\n",
    "1 so this is the kind of food that will kill you so there s t...\n",
    "```\n",
    "\n",
    "In order to transform the set of words into vectors, we will rely on a method of feature engineering called word embeddings. Rather than simply indicating which words are present, word embeddings represent each word by \"embedding\" it in a low-dimensional vector space which may carry more information about the semantic meaning of the word. (for example in this space, the words \"King\" and \"Queen\" would be close).\n",
    "\n",
    "`word2vec.txt` contains the `word2vec` embeddings for about 15 thousand words. Not every word in each review is present in the provided `word2vec.txt` file. We can treat these words as being \"out of vocabulary\" and ignore them.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let x_i denote the sentence `“a hot dog is not a sandwich because it is not square”` and let a toy word2vec dictionary be as follows:\n",
    "\n",
    "```\n",
    "hot      0.1     0.2     0.3\n",
    "not      -0.1    0.2     -0.3\n",
    "sandwich 0.0     -0.2    0.4\n",
    "square   0.2     -0.1    0.5\n",
    "```\n",
    "\n",
    "we would first `trim` the sentence to only contain words in our vocabulary: `\"hot not sandwich not square”` then embed x_i into the feature space:\n",
    "\n",
    "$$ φ2(x_i)) = \\frac{1}{5} (word2vec(\\text{hot}) + 2 · word2vec(\\text{not}) + word2vec(\\text{sandwich}) + word2vec(\\text{square})) = \\left[0.02 \\hspace{2mm} 0.06 \\hspace{2mm} 0.12 \\hspace{2mm}\\right]^T $$\n",
    "\n",
    "### Part 1 (20pts)\n",
    "\n",
    "a) Implement a function to trim out-of-vocabulary words from the reviews. Your function should return an nd array of the same dimension and dtype as the original loaded dataset. (10pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "VECTOR_LEN = 300   # Length of word2vec vector\n",
    "MAX_WORD_LEN = 64  # Max word length in dict.txt and word2vec.txt\n",
    "\n",
    "################################################################################\n",
    "# We have provided you the functions for loading the tsv and txt files. Feel   #\n",
    "# free to use them! No need to change them at all.                             #\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def load_tsv_dataset(file):\n",
    "    \"\"\"\n",
    "    Loads raw data and returns a tuple containing the reviews and their ratings.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): File path to the dataset tsv file.\n",
    "\n",
    "    Returns:\n",
    "        An np.ndarray of shape N. N is the number of data points in the tsv file.\n",
    "        Each element dataset[i] is a tuple (label, review), where the label is\n",
    "        an integer (0 or 1) and the review is a string.\n",
    "    \"\"\"\n",
    "    dataset = np.loadtxt(file, delimiter='\\t', comments=None, encoding='utf-8',\n",
    "                         dtype='l,O')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_feature_dictionary(file):\n",
    "    \"\"\"\n",
    "    Creates a map of words to vectors using the file that has the word2vec\n",
    "    embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): File path to the word2vec embedding file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary indexed by words, returning the corresponding word2vec\n",
    "        embedding np.ndarray.\n",
    "    \"\"\"\n",
    "    word2vec_map = dict()\n",
    "    with open(file) as f:\n",
    "        read_file = csv.reader(f, delimiter='\\t')\n",
    "        for row in read_file:\n",
    "            word, embedding = row[0], row[1:]\n",
    "            word2vec_map[word] = np.array(embedding, dtype=float)\n",
    "    return word2vec_map\n",
    "\n",
    "\n",
    "def trim_reviews(path_to_dataset):\n",
    "    return\n",
    "\n",
    "trim_train = trim_reviews(\"./data/train_small.tsv\")\n",
    "trim_test = trim_reviews(\"./data/test_small.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Implement the embedding and store it to a `.tsv` file where the first column is the label and the rest are the features from the embedding. Round all numbers to 6 decimal places. `embedded_train_small.tsv` and `embedded_test_small.tsv` contain the expected output of your function. (10pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_reviews(trimmed_dataset):\n",
    "    return\n",
    "\n",
    "def save_as_tsv(dataset, filename):\n",
    "    with open(filename, 'w+') as f:\n",
    "        f.writelines(...)\n",
    "    return\n",
    "\n",
    "embedded_train = embed_reviews(trim_train)\n",
    "embedded_test = embed_reviews(trim_test)\n",
    "\n",
    "save_as_tsv(embedded_train, \"./data/output/embedded_train_small.tsv\")\n",
    "save_as_tsv(embedded_test, \"./data/output/embedded_test_small.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 (20pts)\n",
    "\n",
    "In this part we'll be implementing Gradient Descent for binary Logistic Regression Classifier.\n",
    "\n",
    "Some rules:\n",
    "\n",
    "1. Include an intercept term in your model. You must consider the bias term as part of the weight vector and not a separate term to keep track of.\n",
    "2. Initialize all model parameters to 0\n",
    "3. Use vector and matrix multiplication\n",
    "\n",
    "The expected `metrics.txt` from the dataset with `500` epochs and `0.001` learning rate is:\n",
    "\n",
    "```\n",
    "error(train): 0.000000\n",
    "error(test): 0.625000\n",
    "```\n",
    "\n",
    "We will be testing your code on other, larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_tsv_dataset(file):\n",
    "    return np.loadtxt(file, delimiter='\\t', encoding='utf-8')\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return ...\n",
    "\n",
    "\n",
    "def gd(theta, X, y, learning_rate):\n",
    "    # TODO: implement in vector form\n",
    "    return theta\n",
    "\n",
    "\n",
    "def train(theta, X, y, num_epoch, learning_rate):\n",
    "    ...\n",
    "    return theta\n",
    "\n",
    "\n",
    "def predict(theta, X):\n",
    "    # TODO: implement in vector form\n",
    "    return\n",
    "\n",
    "\n",
    "def compute_error(y_pred, y):\n",
    "    # TODO: implement in vector form\n",
    "    return\n",
    "\n",
    "\n",
    "def write_metrics(train_err, test_err, metrics_out):\n",
    "    with open(metrics_out, 'w+') as f:\n",
    "        w = \"error(train): \" + \"{:.6f}\".format(train_err) + \"\\n\"\n",
    "        w += \"error(test): \" + \"{:.6f}\".format(test_err) + \"\\n\"\n",
    "        f.write(w)\n",
    "    return\n",
    "\n",
    "\n",
    "def logistic_reg(formatted_train, formatted_test, metrics_out, num_epochs, learning_rate):\n",
    "    theta = ...\n",
    "    y = ...\n",
    "    X = ...\n",
    "\n",
    "    learned_theta = train(theta, X, y, num_epochs, learning_rate)\n",
    "    train_pred = predict(learned_theta, X)\n",
    "    train_err = compute_error(train_pred, y)\n",
    "\n",
    "    X_test = ...\n",
    "    y_test = ...\n",
    "    test_pred = predict(learned_theta, X_test)\n",
    "    test_err = compute_error(test_pred, y_test)\n",
    "\n",
    "    write_metrics(train_err, test_err, metrics_out)\n",
    "    return\n",
    "\n",
    "\n",
    "logistic_reg(\"./data/embedded_train_small.tsv\", \"./data/embedded_test_small.tsv\", \"./data/output/metrics.txt\", 500, 0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (60 Points)\n",
    "\n",
    "For this exercise, recall the lab's kaggle competition where you predicted if a cookie was defective or not (assuming you attend labs). For more information on the data and details, [click here to view the competition and data details](https://www.kaggle.com/competitions/cs506-lab-defective-cookie-detection/data)\n",
    "\n",
    "However, what we will do differently here is check if two of the features were correlated. This is important because if two features are correlated, then we can remove one of the features and still get the same information. This is part of the feature selection process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the Data\n",
    "\n",
    "For this exercise, we are interested in the two features `Texture Hardness` and `Texture Chewiness` and want to see if they are related. To do this, we will load in the data and create a scatter plot of the two features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** (1 Point) Download the kaggle data from the link above and read in the `csv` files `cookie_train.csv` and `cookie_test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Training and Testing Data\n",
    "\n",
    "The lab did not have a lot of cookies for the training set, so what we will do is combine the training and testing data together without any loss of generality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** (3 Point) Combine the training and testing data together into a single dataframe called `cookie_data` and plot the scatter plot for `Texture Chewiness` (our y or our response variable) and `Texture Hardness` (our x or our explanatory variable). Make sure to label your axes and give your plot a title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking If We Can Do Linear Regression\n",
    "\n",
    "It is important to check all requirements/assumptions for linear regression are met before we do linear regression. One of the requirements is that the explanatory variable and response variable are linearly related, which we checked by noticing the scatter plot was linear. However, there is much more to check!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for Normality\n",
    "It is important to check that the response variable is normally distributed. This is important for hypothesis testing and relates to the idea of linear regression being an unbiased, minimum variance estimator (if you're interested in this, check out the course CAS MA 582).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) (5 Points)** Plot an appropriate graph to determine if the response variable `Texture Chewiness` follows the normality assumption. State whether the normality assumption is met or not. If it is not met, state what can we do to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot in this cell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Written response goes in this cell)_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for Homoscedasticity\n",
    "It is important to check that the variance of the response variable is the same across all values of the explanatory variable. Otherwise, the confidence around a predicted value of the response variable will vary depending on the explanatory variable value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) (5 Points)** Plot an appropriate graph to determine if the response variable `Texture Chewiness` follows the homoscedasticity assumption. State whether the homoscedasticity assumption is met or not. If it is not met, state what can we do to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot in this cell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Written response goes in this cell)_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for Independence\n",
    "It is important to check that the explanatory variable and response variable are independent. Otherwise, the model isn't really best fit with a line. To do so, we will use the Durbin-Watson test.\n",
    "\n",
    "**e) (5 Points)** Use the Durbin-Watson test to determine if the explanatory variable and response variable are independent. State whether if the independence assumption is met or not. If it is not met, state what can we do to fix this. You may use any library to do this, and for more information, check out [this link](https://www.investopedia.com/terms/d/durbin-watson-statistic.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this cell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Written response goes in this cell)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting the Model\n",
    "If you think any of the assumptions are not met, then you should respectively fix the model.\n",
    "\n",
    "**f) (6 Points)** If you think atleast one of the assumptions were not met, then adjust the model how you described you would. Otherwise, proceed to the next step and in the cell below, write \"No adjustments were made to the model\". However, if you do adjust the model, then in the cell below, justify why your adjustments is appropriate (i.e. plot the new graphs and statistics found above). You may add as many cells as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this cell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing\n",
    "Now that we have a model that is appropriate, we can do hypothesis testing. We will be using the Linear Regression t-test to test if the explanatory variable is a significant predictor of the response variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g) (5 Points)** Use the Linear Regression t-test to determine if the explanatory variable `Texture Hardness` is a significant predictor of the response variable `Texture Chewiness`. State whether if the explanatory variable is a significant predictor of the response variable or not using an alpha significance level of $\\alpha = 0.05$, the null hypothesis $\\beta_{hardness} = 0$ and alternative hypothesis $\\beta_{hardness} \\neq 0$. If it is not, state what can we do to fix this. You may use any library to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in this cell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Written response goes in this cell)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a New Feature\n",
    "Now that we have determined that the explanatory variable is a significant predictor of the response variable, you noticed the fit isn't 100% perfect still (e.g. your model is not capturing all the variation in the data). You think that there is another feature that can help improve the model. You think that the feature `Taste Sweetness` can help improve your model. You want to test if this is true. You may assume that all the assumptions are met for this new model to conduct linear regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h) (10 Points)** Now that you have two features to predict your response variable, you will have to use multiple linear regression. Use the `statsmodels` library to do multiple linear regression. Then, determine if the explanatory variables `Texture Hardness` and `Taste Sweetness` are significant predictors of the response variable `Texture Chewiness` using an alpha significance level of $\\alpha = 0.001$, the null hypothesis $\\beta_{hardness} = 0$ and $\\beta_{sweetness} = 0$ and alternative hypothesis $\\beta_{hardness} \\neq 0$ and $\\beta_{sweetness} \\neq 0$. If it is not, state what can we do to fix this. You may use any library to do this.\n",
    "\n",
    "Moreover, justify whether the overall model is significant or not using the F-test with the same $\\alpha$ given above. If it is not, state what can we do to fix this.\n",
    "\n",
    "_You may use the transformed model made (if one was made) from the previous parts and add this new feature to it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in this cell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Written response goes in this cell)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "It is important to be able to interpret the results of the model. This is important because it allows us to understand the model and how it works. It also allows us to understand the relationship between the predictors and the response variable, and this applies to not just linear regression, but all learning models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i) (10 Points)** Interpret the coefficients of the model made in part **h**. What do they mean? What do they represent? What do they tell us about the relationship between the predictors and the response variable? If your code output in part h does not provide these results, you are welcome to use any library here to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in this cell if you need it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Written response goes in this cell)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Taste Bitterness over Taste Sweetness\n",
    "You think that the feature `Taste Bitterness` can help improve your model since `Taste Sweetness` wasn't it.\n",
    "\n",
    "For this section, you will continue to use your fixed/adjusted features for `Texture Hardness` and `Texture Chewiness`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**j) (10 Points)** Plot 2 scatter plots: `Taste Bitterness` vs your possibly adjusted `Texture Chewiness` and `Taste Bitterness` vs your possibly adjusted `Texture Hardness`. Given these plots, propose an appropriate model. Justify your answer by fitting it and seeing its accuracy (you can use any metric to prove so), showing appropriate models/graphs, and ensuring assumptions are satisfied (if any). You may use any library to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (20pts)\n",
    "\n",
    "Lasso Regression is a modification of Regression that adds a regularization penalty to the the parameter $\\mathbf{\\beta}$ learned by the model. The loss function for Lasso is the following:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{\\beta}) = \\Vert \\mathbf{y} - X\\mathbf{\\beta} \\Vert^2 + \\lambda \\Vert \\mathbf{\\beta} \\Vert = \\beta^T X^T X \\beta - 2\\mathbf{\\beta}^TX^T\\mathbf{y}  + \\mathbf{y}^T\\mathbf{y} + \\lambda \\Vert \\mathbf{\\beta} \\Vert$$\n",
    "\n",
    "Where $\\lambda$ is a tuning parameter specified by the user.\n",
    "\n",
    "Using worksheet 17 as a guide:\n",
    "\n",
    "1. create an animation like in part c) for Lasso Regression. [15pts]\n",
    "2. create at least two more animations using different values for $\\lambda$ and the true $\\mathbf{\\beta}$ you used to generate the dataset (try steeper / less steep curves, and higher or close to zero intercepts). Briefly explain some key takeaways on how to tune $\\lambda$. [5pts]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
